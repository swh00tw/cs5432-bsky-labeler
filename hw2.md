Tackling a Coordinated Harassment Harm on BlueSky:
Dogpiling with Linguistic Evasion

BlueSky has established its reputation as a safer, more community-friendly alternative
to X. However, in the year that it has been a publicly available platform (since February
2024), BlueSky has not been immune to coordinated harassment attacks. This policy
proposal considers how dogpiling with linguistic evasion manifests on BlueSky, and
why and how we can help combat it.

Defining the Harm
Background: Dogpiling is a form of coordinated harassment where a large group of
abusers collectively attack a single individual (target) with threats, slurs, insults, and
other abusive tactics.1 This creates an overwhelming and intimidating experience for the
target, who faces a disproportionate volume of negative interactions that can lead to
psychological distress, platform abandonment, and chilling effects on free expression.
On BlueSky, this harm is especially concerning when bad actors employ sophisticated
linguistic evasion tactics to bypass automated content moderation systems, including:

1.  Homophones: Using words that sound like prohibited terms but are spelled
    differently (e.g., "dye" instead of "die")
2.  Spoonerisms: Switching initial sounds of words to obscure an expletive (e.g.,
    "huck farvard" instead of ”fuck harvard”)
3.  Character substitution: Replacing letters with similar-looking characters (e.g.,
    "f@ggot")
4.  Deliberate typos: Intentionally misspelling words to avoid detection while
    maintaining recognizable meaning (e.g., “fuq”)
5.  Coded language: Developing in-group terminology that carries harassing
    meaning while appearing benign to outsiders (e.g., “41%” as a reference to
    suicide statistics among trans populations)
    To crystalize the definition of this joint harm, dogpiling with linguistic evasion is the
    coordinated targeting of individuals on a platform with harassing language that
    bypasses most standard automated content moderation systems.  
    Examples of this Harm
    In 2024, journalist Jesse Singal became the target of dogpiling on Bluesky for his
    articles about youth trans healthcare.2 Prominent LGBTQ+ organizations like GLAAD
    and publications like Slate characterized Singal’s journalism as harmful to trans
    communities years prior 3 4, and trans activists have called for his deplatforming in the
    CS 5342: Trust and Safety: Platforms, Policies, Products
    Team 17: Priyanshi Gupta (pg485), Frank Hsu (sh2575), Nikhil Jain (nj289)
    wake of increasing attacks on trans youth across the US. When he joined Bluesky in
    2024, he did not, however, expect to be the target of death threats. Here are a few
    examples of the harassment he faced that can be considered dogpiling with linguistic
    evasion, regardless of one’s agreement, or lack thereof, with his broader journalistic
    ethics and politics:

User Beastie enby’s comment “Sill Kingal” is a clear example of spoonerism (like the
original poster’s username Bill Kezos, spoonerism of Kill Bezos) that may easily be
missed by a content moderation algorithm that is checking for cases of the word “Kill”
next to a person’s name. Other comments in this screenshot similarly may not trigger
content moderation but, especially in the context of hundreds of other posts like this,
clearly classify as coordinated harassment. Similar methods have been used to dogpile
against journalists like Jamelle Bouie, who reportedly left Bluesky after being harassed
for publishing centrist views in his New York Times articles.5

CS 5342: Trust and Safety: Platforms, Policies, Products
Team 17: Priyanshi Gupta (pg485), Frank Hsu (sh2575), Nikhil Jain (nj289)
Below are examples of dogpiling with linguistic evasion that we have found or
generated, some with the support of AI after inputting our definition of the harm, as well
as some examples that are out of scope:

1.  Spoonerism-based death threats: Examples from above (bill kezos, sill kingal),
    especially in association with posting of real addresses or personal details.
2.  Multiple users flooding a target’s post with homophonic harassment:
    Multiple comments in a short time frame on a user’s post saying things like “I
    hope you dye” or “you should commit sewer slide”
3.  Character substitution in coordinated attacks: A female game developer
    releases a new title and receives hundreds of messages like: "g0 b@ck to the
    k1tchen"
4.  Deliberately misspelled slurs in mass harassment: A racial minority content
    creator receives dozens of comments containing variations like "n1gr," "neegr0,"
    and "knee-grow" by a set of users who frequently post these phrases.
5.  Combined tactics in overwhelming volume: A trans activist receives hundreds
    of messages within hours combining multiple evasion techniques: "41%",
    "gr00m3r," "m3nt@lly !ll,” etc.

Out-of-scope examples:

1.  Blocklists and labels targeting an individual, but unrelated to text-based
    dogpiling: Multiple accounts seek to target Jesse Singal in non-textual ways,
    like offering a labeler to indicate when someone follows Singal or automatically
    blocking them, but these do not fall into our harm category since we are focused
    specifically on text-based dogpiling harms.
2.  Text-based harassment without linguistic evasion tactics: A user posts
    "you're a stupid bitch" on a targeted individual’s post - offensive but using
    straightforward language without attempts to evade detection.

Framing the ABCs of this Harm
Dogpiling with linguistic evasion is typically perpetrated as follows:
● Actors: Malicious users who seek to harass individuals, coordinated bot
networks.
● Behaviors: Sharing harassing posts about a targeted user, adding harassing
comments on user’s posts, sending harassing messages to user, sharing user’s
exact address or other personal information.
● Content: The abusive messages that are produced with evasion techniques.

CS 5342: Trust and Safety: Platforms, Policies, Products
Team 17: Priyanshi Gupta (pg485), Frank Hsu (sh2575), Nikhil Jain (nj289)
Justification for Intervention
Addressing dogpiling is critical for Bluesky, given both the severity of its societal impact
and industry standards set by competitor platforms. Specialized international
institutions, academic researchers, and safety organizations emphasize the severe
consequences of coordinated harassment, including psychological harm, suppression of
free expression, and user attrition.

Societal Risks:  
The UN Special Rapporteur on Freedom of Opinion and Expression highlights
coordinated harassment as a serious threat to online expression, especially impacting
marginalized communities. Users report heightened feelings of anxiety, depression, and
even post-traumatic stress disorder (PTSD) from dogpiling attacks. At a time when over
half (52%) of Americans experienced online harassment as of 2023, there is significant
prevalence and urgency for addressing this harm, especially when linguistic evasion
tactics are employed, which was the case in 30% of harassment attacks. These tactics
significantly reduce the ability for content moderation algorithms to moderate the attacks
in a timely fashion. 6 7 8

Business Impact:  
Leading social media platforms including Twitter, Facebook, Reddit, and Discord have
publicly prioritized advanced, context-aware moderation strategies. Twitter has invested
in natural language processing-based detection systems to combat linguistic evasion
and reduce coordinated attacks. Facebook’s Meta employs advanced phonetic and
pattern-matching moderation techniques designed to identify near-miss abusive terms.
Similarly, Discord utilizes sophisticated behavioral analytics combined with
human-in-the-loop moderation tools to rapidly identify and mitigate coordinated
harassment.
Given this competitive landscape, it is strategically important for BlueSky to match or
surpass these industry benchmarks. Proactively addressing coordinated harassment
through advanced linguistic detection and behavioral moderation methods not only
aligns with best practices but is essential to preserving the platform’s user trust and
competitive edge. Platforms that fail to adequately tackle coordinated harassment have
historically experienced negative repercussions, including declining user retention,
reputational damage, and decreased advertising revenue.
By investing now in robust detection and moderation capabilities, BlueSky positions
itself as a safer, healthier alternative to incumbent social media networks. This aligns
directly with the company’s strategic imperative to attract users seeking safer and more
trustworthy digital communities, ultimately driving sustainable user growth, enhancing
advertiser appeal, and securing long-term market viability. 9 10
CS 5342: Trust and Safety: Platforms, Policies, Products
Team 17: Priyanshi Gupta (pg485), Frank Hsu (sh2575), Nikhil Jain (nj289)
Proposed Solution
Technical Overview: Our solution leverages modern machine learning, behavioral
analytics, pattern recognition, and API integration to detect coordinated dogpiling
employing linguistic evasion. We combine a Natural Language Processing (NLP)
approach for detecting linguistic variations of harmful content with a data science
approach to identifying clusters of potentially harmful behaviors (e.g., multiple posts
mentioning a user, sharing an address string, or being posted alongside a labeler that is
associated with a real person’s name in a short time frame). Through this combined
approach, we aim to mitigate both the dogpiling and the linguistic evasion that are
problematic when occurring together.
(1) Data Ingestion and Preprocessing
● Real-time Streaming: Continuously ingest posts and metadata via Bluesky’s
APIs (RESTful/WebSocket, Python AT Protocol SDK).
● Text Normalization: Lowercase conversion, punctuation removal, Unicode
normalization via spaCy and Hugging Face.
● Feature Extraction: Generate embeddings using Sentence Transformers or
fastText.
(2) Automated Detection Modules
A. Pattern Matching
● Phonetic Matching: Algorithms (Soundex, Metaphone) identify phonetic variants
of abusive terms.
● Edit Distance and Homoglyph Detection: RapidFuzz for Levenshtein
distances and Unicode homoglyph normalization detect deceptive substitutions.
B. Context-Aware NLP
● Transformer Models: Fine-tuned BERT/RoBERTa models analyze contextual
and semantic intent of potentially evasive content.
● Ensemble Decision System: Weighted integration of pattern matching and NLP
confidence scores. Content flagged if confidence exceeds thresholds (80-95%).
C. Behavioral and Network Analysis
● Dynamic Trust Scoring: Real-time updates based on moderation history,
posting patterns, and behavior.
● Graph-Based Clustering: DBSCAN identifies coordinated attack patterns within
short timeframes, highlighting orchestrated harassment.
CS 5342: Trust and Safety: Platforms, Policies, Products
Team 17: Priyanshi Gupta (pg485), Frank Hsu (sh2575), Nikhil Jain (nj289)
(3) Moderation Integration
● Automated Flagging: High-confidence (>95%) flagged content temporarily
suppressed via Bluesky API.
● Human-in-the-Loop Review: Moderators utilize a dashboard (FastAPI backend,
React frontend) to confirm or dismiss flagged content, refining detection models
with human oversight via MLflow.

Workflow Diagram:

CS 5342: Trust and Safety: Platforms, Policies, Products
Team 17: Priyanshi Gupta (pg485), Frank Hsu (sh2575), Nikhil Jain (nj289)

In addition to technical solutions employed within Bluesky’s infrastructure, we
considered some behavioral nudges that users may be guided to:
● In-App Notifications: Real-time reminders of community guidelines when
abusive behavior is detected.
● Posting Restrictions: Temporary limitations placed on repeat offenders.
● Positive Reinforcement: Incentives for maintaining high trust scores to promote
positive community behaviors.
Additionally, we have listed the alternative solutions that we considered but chose not to
pursue:
● Keyword-based moderation: Ineffective due to linguistic evasion complexity.
● Pure manual moderation: Unsustainable at scale.
● Restrictive automation: Risks excessive false positives, potentially stifling
legitimate speech.
Feasibility
Leveraging open-source libraries (spaCy, Hugging Face, RapidFuzz, scikit-learn) and
proven moderation frameworks (Hasher Matcher Action by Meta, Perspective API by
Jigsaw) makes implementation feasible and cost-effective. Cloud infrastructure (AWS,
Google Cloud) and container orchestration (Kubernetes) enable scalability.

Impact Measurement
We propose four broad categories to track the success of this policy proposal:
● Accuracy Metrics: Track precision, recall, and false-positive rates.
● Moderation Response Latency: Time from detection to human review
completion.
● User Metrics: Engagement levels, retention rates, incident volume reports.
● Trust Score Dynamics: Evaluate user behavior improvements.
Risks & Considerations
While the moderation tools and associated sets of policies listed here can go a long way
in mitigating evasive dogpiling, it can also be perceived as limiting free expression. We
consider it important to note that using spoonerisms of “Kill [Name]” are different when
targeting an individual that is generally vulnerable and at real security risk (e.g., a
journalist or film maker) vs. using it as an expression of anger at someone in
considerable power and protection (e.g., a president or the CEO of a large company).
As a result, our solution would need to account for exceptions to , which we may be
CS 5342: Trust and Safety: Platforms, Policies, Products
Team 17: Priyanshi Gupta (pg485), Frank Hsu (sh2575), Nikhil Jain (nj289)
able to gather from (a) user input through their challenging of takedowns, and (b)
external guidance on forms of political expression that should not be considered real
threats (e.g., drawing a parallel to how violent lyrics in diss tracks on Spotify are not
censored because they are considered an artistic part of the hip hop tradition, per Jerrel
Peterson’s lecture).

Additionally, there must be methods to ensure fairness of the tool’s moderation. If, for
instance, the tool appears to most dominantly protect prominent right wing voices from
evasive dogpiling attacks, we can expect Bluesky’s users to see this tool as politically
biased. Bluesky has already received considerable criticism from some of its users for
taking moderation actions against the accounts that harassed Jesse Singal. To be able
to enforce and maintain these actions, Bluesky would need to be able to verify that it
takes similar actions across the spectrum of political and demographic identities.

Finally, we recognize that this solution employs largely technical, automated
approaches to content moderation, with limited discussion of the kinds of policy
exceptions that may require manual methods. This is where we expect Bluesky would
rely on tools and features for users to report accounts, posts, labelers, and messages to
flag for human moderation. If the tool’s automated workflows successfully reduce the
volume of evasive harassment, it frees up considerable bandwidth for Trust & Safety
teams at Bluesky to focus on the unique edge cases (e.g., in dogpiling of political
entities) and to consider how to keep its policies robust and iterative.
